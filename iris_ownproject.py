# -*- coding: utf-8 -*-
"""Iris_OwnProject

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Q2D4GAPOWc-WJ4JXpzsN8woBceus4RbG
"""

# Install sklearn and from sklearn library import iris dataset
# Load_iris is a function provided to us from sklearn library
from sklearn.datasets import load_iris

# Used to generate plots
import matplotlib.pyplot as plt

# Used to split training and testing data
from sklearn.model_selection import train_test_split

# Used for knn
from sklearn.neighbors import KNeighborsClassifier
import numpy as np

# Storing iris dataset
iris = load_iris()

# Transpose of the iris dataset to get each of the feature in separate lists
features = iris.data.T

# Storing each of the features into separate list
sepal_length = features[0]
sepal_width = features[1]
petal_length = features[2]
petal_width = features[3]

# Storing each of the feature name from dictonary with key features_names
sepal_length_label = iris.feature_names[0]
sepal_width_label = iris.feature_names[1]
petal_length_label = iris.feature_names[2]
petal_length_label = iris.feature_names[3]

# Plotting scatter plot 
plt.scatter(sepal_length,petal_length,c=iris.target)
plt.xlabel(sepal_length_label)
plt.ylabel(sepal_width_label)

# Displaying scatter plot
plt.show()

# Spliting dataset into training and testing dataset
# Use train/test split with different random state values
# The accuracy changes a lot 
# This is why testing accuracy is a high-variance estimate
X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=0)

# Classification with K=5
knn = KNeighborsClassifier(n_neighbors=5)

# Training the model
knn.fit(X_train,y_train)

# Reshaping as knn always takes 2D array as input
# Imagine X_new has a new data point to be classified
X_new = np.array([5,2.9,1,0.2]).reshape(1,-1)
print(X_new.shape)

# Predicting target value for the new data point
prediction = knn.predict(X_new)
print(prediction)

# Printing the accuracy of the model
print(knn.score(X_test,y_test))

# Cross-Validation for Parameter Tuning,Model Selection, and Feature Selection

from sklearn import metrics

# What if we created a bunch of train/test splits, calculated the testin g accuracy for each, and averaged the results together? This is the essense of cross-validation
# Steps for K-fold cross-validation
# 1. Split the dataset into K equal partitions(or folds)
        # So if k=5 and dataset has 150 observations
        # Each of the 5 folds would have 30 observations
# 2. Use fold 1 as the testing set and the union of the other folds as the training set
        # Testing set = 30 observations(fold 1)
        # Training set = 120 observations(fold 2-5)
# 3. Calculate testing accuracy 
# 4. Repeat steps 2 and 3 K times, using different fold as the testing set each time 
        # We will repeat the process 5 times
        # 2nd iteration 
            # fold 2 will be testing set 
            # union of fold 1,3,4 and 5 would be the training set

            # and so on...

